{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Feature-extraxtor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "-QX4orDJxXXF"
      },
      "outputs": [],
      "source": [
        "def clientPE(pefile):\n",
        "    \n",
        "    import os.path\n",
        "    if os.path.isfile(pefile):    \n",
        "        sample_data = open(pefile, \"rb\").read()\n",
        "        extractor = PEFeatureExtractor(2)\n",
        "        sample_data = np.array(extractor.feature_vector(sample_data), dtype=np.float32)\n",
        "        sample_data = ss.fit_transform([sample_data])\n",
        "        pred = loaded_model.predict(sample_data)\n",
        "        pred = pred*100\n",
        "        print(pred)\n",
        "        print(\"\\n\\n\")\n",
        "        if pred[0][0]< 50:\n",
        "            print(f\"file: {pefile} is benign\")\n",
        "        else:\n",
        "            print(f\"file: {pefile}  is malacious\")\n",
        "\n",
        "    else:\n",
        "        \n",
        "        print(\"File dont exists\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Engineering\n",
        "#we want to standardize the feature, we can use MinMaxScaler like we have used in previous calsses\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "ss =RobustScaler()"
      ],
      "metadata": {
        "id": "dw4AOvx61tjH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.models import model_from_json"
      ],
      "metadata": {
        "id": "Iw3bo7pF6lUu"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open('malModel.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json,custom_objects={\"GlorotUniform\": tf.keras.initializers.glorot_uniform})"
      ],
      "metadata": {
        "id": "5gy8t6Yp6yiR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model.load_weights('model_weight_1.h5')"
      ],
      "metadata": {
        "id": "kpcRqVJy63lx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "feature_size=2381\n",
        "win_size=500\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "keras.backend.clear_session()\n",
        "from tensorflow.keras import layers"
      ],
      "metadata": {
        "id": "9LfoGJXs0bTY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abcd = tf.keras.models.load_model('model_1.h5')"
      ],
      "metadata": {
        "id": "FSTjfk2szK6z"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abcd.load_weights('model_weight_1.h5')"
      ],
      "metadata": {
        "id": "wq9aks4vdF3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "TWVBVAqExdSn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ember"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47irmqNhyec8",
        "outputId": "bdc3f4a7-f09b-4cf1-ec89-6d3fc5a4783e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ember in /usr/local/lib/python3.7/dist-packages (0.1.0)\n",
            "Requirement already satisfied: lightgbm>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from ember) (2.2.3)\n",
            "Requirement already satisfied: tqdm>=4.31.0 in /usr/local/lib/python3.7/dist-packages (from ember) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from ember) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from ember) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from ember) (1.0.2)\n",
            "Requirement already satisfied: lief>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from ember) (0.12.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.2.3->ember) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->ember) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->ember) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->ember) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->ember) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->ember) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip uninstall ember"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtuYs6MuxgKj",
        "outputId": "7ed2b1aa-1f58-47d0-d202-4080a0d6a857"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: ember 0.1.0\n",
            "Uninstalling ember-0.1.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/ember-0.1.0.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/ember/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled ember-0.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install git+https://github.com/elastic/ember.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWpMj3HGxtJl",
        "outputId": "60ea92b2-058c-4d3f-e6fb-35145478ec73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/elastic/ember.git\n",
            "  Cloning https://github.com/elastic/ember.git to /tmp/pip-req-build-a179yvm2\n",
            "  Running command git clone -q https://github.com/elastic/ember.git /tmp/pip-req-build-a179yvm2\n",
            "Collecting lief>=0.9.0\n",
            "  Downloading lief-0.12.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.31.0 in /usr/local/lib/python3.7/dist-packages (from ember==0.1.0) (4.64.0)\n",
            "Requirement already satisfied: numpy>=1.16.3 in /usr/local/lib/python3.7/dist-packages (from ember==0.1.0) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.7/dist-packages (from ember==0.1.0) (1.3.5)\n",
            "Requirement already satisfied: lightgbm>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from ember==0.1.0) (2.2.3)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from ember==0.1.0) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from lightgbm>=2.2.3->ember==0.1.0) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->ember==0.1.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.2->ember==0.1.0) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.2->ember==0.1.0) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->ember==0.1.0) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.3->ember==0.1.0) (3.1.0)\n",
            "Building wheels for collected packages: ember\n",
            "  Building wheel for ember (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ember: filename=ember-0.1.0-py3-none-any.whl size=12845 sha256=0ae06e2150f1a0318aaea2f40b732a8a8cfeb6e6fb5001aef33111018b6f31bd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-fctputu0/wheels/0d/0c/d5/8bf798260ec4c5ab76e1cd3aef7489cd84fe1a4284ef80108b\n",
            "Successfully built ember\n",
            "Installing collected packages: lief, ember\n",
            "Successfully installed ember-0.1.0 lief-0.12.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ember"
      ],
      "metadata": {
        "id": "nx64dTfL4_7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clientPE('AnyDesk.exe')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dnVe-jB4xx-L",
        "outputId": "0084c180-660f-4291-8528-3084c5fdd8f5"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\n",
            "WARNING:   lief version 0.10.0-845f675 found instead. There may be slight inconsistencies\n",
            "WARNING:   in the feature calculations.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:88: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[56.978954]]\n",
            "\n",
            "\n",
            "\n",
            "file: AnyDesk.exe  is malacious\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_v1.py:2079: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
            "  updates=self.state_updates,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_file = open('keras_model/malModel.json', 'r')\n",
        "loaded_model_json = json_file.read()\n",
        "json_file.close()\n",
        "loaded_model = model_from_json(loaded_model_json,custom_objects={\"GlorotUniform\": tf.keras.initializers.glorot_uniform})"
      ],
      "metadata": {
        "id": "jVhjnjYCezT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ember"
      ],
      "metadata": {
        "id": "mjhR7XfX2SvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ember"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Azw6J5B2yf_",
        "outputId": "ecd03eaa-3307-444a-dfa5-e2cbbe133c87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement ember (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for ember\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3vz1FT0N21L0",
        "outputId": "e092ffcd-6b15-42f9-92d9-b155496a253d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os,json"
      ],
      "metadata": {
        "id": "ny-cHnSl7X0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import hashlib"
      ],
      "metadata": {
        "id": "RE1px8kx7aMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import lief"
      ],
      "metadata": {
        "id": "FyWtzwod7d_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "5cd42c7d-797a-4099-8538-af3c74b7a72e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-e3d7207213e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mlief\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lief'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PEFeatureExtractor(object):\n",
        "    ''' Extract useful features from a PE file, and return as a vector of fixed size. '''\n",
        "\n",
        "    def __init__(self, feature_version=2, print_feature_warning=True, features_file=''):\n",
        "        self.features = []\n",
        "        features = {\n",
        "                    'ByteHistogram': ByteHistogram(),\n",
        "                    'ByteEntropyHistogram': ByteEntropyHistogram(),\n",
        "                    'StringExtractor': StringExtractor(),\n",
        "                    'GeneralFileInfo': GeneralFileInfo(),\n",
        "                    'HeaderFileInfo': HeaderFileInfo(),\n",
        "                    'SectionInfo': SectionInfo(),\n",
        "                    'ImportsInfo': ImportsInfo(),\n",
        "                    'ExportsInfo': ExportsInfo()\n",
        "            }\n",
        "        \n",
        "        if os.path.exists(features_file):\n",
        "            with open(features_file, encoding='utf8') as f:\n",
        "                x = json.load(f)\n",
        "                self.features = [features[feature] for feature in x['features'] if feature in features]\n",
        "        else:\n",
        "            self.features = list(features.values()) \n",
        "        \n",
        "        if feature_version == 1:\n",
        "            if not lief.__version__.startswith(\"0.8.3\"):\n",
        "                if print_feature_warning:\n",
        "                    print(f\"WARNING: EMBER feature version 1 were computed using lief version 0.8.3-18d5b75\")\n",
        "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
        "                    print(f\"WARNING:   in the feature calculations.\")\n",
        "        elif feature_version == 2:\n",
        "            self.features.append(DataDirectories())\n",
        "            if not lief.__version__.startswith(\"0.9.0\"):\n",
        "                if print_feature_warning:\n",
        "                    print(f\"WARNING: EMBER feature version 2 were computed using lief version 0.9.0-\")\n",
        "                    print(f\"WARNING:   lief version {lief.__version__} found instead. There may be slight inconsistencies\")\n",
        "                    print(f\"WARNING:   in the feature calculations.\")\n",
        "        else:\n",
        "            raise Exception(f\"EMBER feature version must be 1 or 2. Not {feature_version}\")\n",
        "        self.dim = sum([fe.dim for fe in self.features])\n",
        "\n",
        "    def raw_features(self, bytez):\n",
        "        lief_errors = (lief.bad_format, lief.bad_file, lief.pe_error, lief.parser_error, lief.read_out_of_bound,\n",
        "                       RuntimeError)\n",
        "        try:\n",
        "            lief_binary = lief.PE.parse(list(bytez))\n",
        "        except lief_errors as e:\n",
        "            print(\"lief error: \", str(e))\n",
        "            lief_binary = None\n",
        "        except Exception:  # everything else (KeyboardInterrupt, SystemExit, ValueError):\n",
        "            raise\n",
        "\n",
        "        features = {\"sha256\": hashlib.sha256(bytez).hexdigest()}\n",
        "        features.update({fe.name: fe.raw_features(bytez, lief_binary) for fe in self.features})\n",
        "        return features\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        feature_vectors = [fe.process_raw_features(raw_obj[fe.name]) for fe in self.features]\n",
        "        return np.hstack(feature_vectors).astype(np.float32)\n",
        "\n",
        "    def feature_vector(self, bytez):\n",
        "        return self.process_raw_features(self.raw_features(bytez))"
      ],
      "metadata": {
        "id": "VVG7dFP44IDY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install lief==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_8CqlBf7RO5",
        "outputId": "4db3678d-57bf-4120-d024-0f5309bcbdbb"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lief==0.10.0\n",
            "  Downloading lief-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: lief\n",
            "Successfully installed lief-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import lief"
      ],
      "metadata": {
        "id": "XDlDMpqV8CZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import re\n",
        "import lief\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "\n",
        "LIEF_MAJOR, LIEF_MINOR, _ = lief.__version__.split('.')\n",
        "LIEF_EXPORT_OBJECT = int(LIEF_MAJOR) > 0 or ( int(LIEF_MAJOR)==0 and int(LIEF_MINOR) >= 10 )\n",
        "LIEF_HAS_SIGNATURE = int(LIEF_MAJOR) > 0 or (int(LIEF_MAJOR) == 0 and int(LIEF_MINOR) >= 11)\n",
        "\n",
        "\n",
        "class FeatureType(object):\n",
        "    ''' Base class from which each feature type may inherit '''\n",
        "\n",
        "    name = ''\n",
        "    dim = 0\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}({})'.format(self.name, self.dim)\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        ''' Generate a JSON-able representation of the file '''\n",
        "        raise (NotImplementedError)\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        ''' Generate a feature vector from the raw features '''\n",
        "        raise (NotImplementedError)\n",
        "\n",
        "    def feature_vector(self, bytez, lief_binary):\n",
        "        ''' Directly calculate the feature vector from the sample itself. This should only be implemented differently\n",
        "        if there are significant speedups to be gained from combining the two functions. '''\n",
        "        return self.process_raw_features(self.raw_features(bytez, lief_binary))\n",
        "\n",
        "\n",
        "class ByteHistogram(FeatureType):\n",
        "    ''' Byte histogram (count + non-normalized) over the entire binary file '''\n",
        "\n",
        "    name = 'histogram'\n",
        "    dim = 256\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        counts = np.bincount(np.frombuffer(bytez, dtype=np.uint8), minlength=256)\n",
        "        return counts.tolist()\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        counts = np.array(raw_obj, dtype=np.float32)\n",
        "        sum = counts.sum()\n",
        "        normalized = counts / sum\n",
        "        return normalized\n",
        "\n",
        "\n",
        "class ByteEntropyHistogram(FeatureType):\n",
        "    ''' 2d byte/entropy histogram based loosely on (Saxe and Berlin, 2015).\n",
        "    This roughly approximates the joint probability of byte value and local entropy.\n",
        "    See Section 2.1.1 in https://arxiv.org/pdf/1508.03096.pdf for more info.\n",
        "    '''\n",
        "\n",
        "    name = 'byteentropy'\n",
        "    dim = 256\n",
        "\n",
        "    def __init__(self, step=1024, window=2048):\n",
        "        super(FeatureType, self).__init__()\n",
        "        self.window = window\n",
        "        self.step = step\n",
        "\n",
        "    def _entropy_bin_counts(self, block):\n",
        "        # coarse histogram, 16 bytes per bin\n",
        "        c = np.bincount(block >> 4, minlength=16)  # 16-bin histogram\n",
        "        p = c.astype(np.float32) / self.window\n",
        "        wh = np.where(c)[0]\n",
        "        H = np.sum(-p[wh] * np.log2(\n",
        "            p[wh])) * 2  # * x2 b.c. we reduced information by half: 256 bins (8 bits) to 16 bins (4 bits)\n",
        "\n",
        "        Hbin = int(H * 2)  # up to 16 bins (max entropy is 8 bits)\n",
        "        if Hbin == 16:  # handle entropy = 8.0 bits\n",
        "            Hbin = 15\n",
        "\n",
        "        return Hbin, c\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        output = np.zeros((16, 16), dtype=np.int)\n",
        "        a = np.frombuffer(bytez, dtype=np.uint8)\n",
        "        if a.shape[0] < self.window:\n",
        "            Hbin, c = self._entropy_bin_counts(a)\n",
        "            output[Hbin, :] += c\n",
        "        else:\n",
        "            # strided trick from here: http://www.rigtorp.se/2011/01/01/rolling-statistics-numpy.html\n",
        "            shape = a.shape[:-1] + (a.shape[-1] - self.window + 1, self.window)\n",
        "            strides = a.strides + (a.strides[-1],)\n",
        "            blocks = np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)[::self.step, :]\n",
        "\n",
        "            # from the blocks, compute histogram\n",
        "            for block in blocks:\n",
        "                Hbin, c = self._entropy_bin_counts(block)\n",
        "                output[Hbin, :] += c\n",
        "\n",
        "        return output.flatten().tolist()\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        counts = np.array(raw_obj, dtype=np.float32)\n",
        "        sum = counts.sum()\n",
        "        normalized = counts / sum\n",
        "        return normalized\n",
        "\n",
        "\n",
        "class SectionInfo(FeatureType):\n",
        "    ''' Information about section names, sizes and entropy.  Uses hashing trick\n",
        "    to summarize all this section info into a feature vector.\n",
        "    '''\n",
        "\n",
        "    name = 'section'\n",
        "    dim = 5 + 50 + 50 + 50 + 50 + 50\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "\n",
        "    @staticmethod\n",
        "    def _properties(s):\n",
        "        return [str(c).split('.')[-1] for c in s.characteristics_lists]\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        if lief_binary is None:\n",
        "            return {\"entry\": \"\", \"sections\": []}\n",
        "\n",
        "        # properties of entry point, or if invalid, the first executable section\n",
        "        try:\n",
        "            entry_section = lief_binary.section_from_offset(lief_binary.entrypoint).name\n",
        "        except lief.not_found:\n",
        "            # bad entry point, let's find the first executable section\n",
        "            entry_section = \"\"\n",
        "            for s in lief_binary.sections:\n",
        "                if lief.PE.SECTION_CHARACTERISTICS.MEM_EXECUTE in s.characteristics_lists:\n",
        "                    entry_section = s.name\n",
        "                    break\n",
        "\n",
        "        raw_obj = {\"entry\": entry_section}\n",
        "        raw_obj[\"sections\"] = [{\n",
        "            'name': s.name,\n",
        "            'size': s.size,\n",
        "            'entropy': s.entropy,\n",
        "            'vsize': s.virtual_size,\n",
        "            'props': self._properties(s)\n",
        "        } for s in lief_binary.sections]\n",
        "        return raw_obj\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        sections = raw_obj['sections']\n",
        "        general = [\n",
        "            len(sections),  # total number of sections\n",
        "            # number of sections with nonzero size\n",
        "            sum(1 for s in sections if s['size'] == 0),\n",
        "            # number of sections with an empty name\n",
        "            sum(1 for s in sections if s['name'] == \"\"),\n",
        "            # number of RX\n",
        "            sum(1 for s in sections if 'MEM_READ' in s['props'] and 'MEM_EXECUTE' in s['props']),\n",
        "            # number of W\n",
        "            sum(1 for s in sections if 'MEM_WRITE' in s['props'])\n",
        "        ]\n",
        "        # gross characteristics of each section\n",
        "        section_sizes = [(s['name'], s['size']) for s in sections]\n",
        "        section_sizes_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_sizes]).toarray()[0]\n",
        "        section_entropy = [(s['name'], s['entropy']) for s in sections]\n",
        "        section_entropy_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_entropy]).toarray()[0]\n",
        "        section_vsize = [(s['name'], s['vsize']) for s in sections]\n",
        "        section_vsize_hashed = FeatureHasher(50, input_type=\"pair\").transform([section_vsize]).toarray()[0]\n",
        "        entry_name_hashed = FeatureHasher(50, input_type=\"string\").transform([raw_obj['entry']]).toarray()[0]\n",
        "        characteristics = [p for s in sections for p in s['props'] if s['name'] == raw_obj['entry']]\n",
        "        characteristics_hashed = FeatureHasher(50, input_type=\"string\").transform([characteristics]).toarray()[0]\n",
        "\n",
        "        return np.hstack([\n",
        "            general, section_sizes_hashed, section_entropy_hashed, section_vsize_hashed, entry_name_hashed,\n",
        "            characteristics_hashed\n",
        "        ]).astype(np.float32)\n",
        "\n",
        "\n",
        "class ImportsInfo(FeatureType):\n",
        "    ''' Information about imported libraries and functions from the\n",
        "    import address table.  Note that the total number of imported\n",
        "    functions is contained in GeneralFileInfo.\n",
        "    '''\n",
        "\n",
        "    name = 'imports'\n",
        "    dim = 1280\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        imports = {}\n",
        "        if lief_binary is None:\n",
        "            return imports\n",
        "\n",
        "        for lib in lief_binary.imports:\n",
        "            if lib.name not in imports:\n",
        "                imports[lib.name] = []  # libraries can be duplicated in listing, extend instead of overwrite\n",
        "\n",
        "            # Clipping assumes there are diminishing returns on the discriminatory power of imported functions\n",
        "            #  beyond the first 10000 characters, and this will help limit the dataset size\n",
        "            for entry in lib.entries:\n",
        "                if entry.is_ordinal:\n",
        "                    imports[lib.name].append(\"ordinal\" + str(entry.ordinal))\n",
        "                else:\n",
        "                    imports[lib.name].append(entry.name[:10000])\n",
        "\n",
        "        return imports\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        # unique libraries\n",
        "        libraries = list(set([l.lower() for l in raw_obj.keys()]))\n",
        "        libraries_hashed = FeatureHasher(256, input_type=\"string\").transform([libraries]).toarray()[0]\n",
        "\n",
        "        # A string like \"kernel32.dll:CreateFileMappingA\" for each imported function\n",
        "        imports = [lib.lower() + ':' + e for lib, elist in raw_obj.items() for e in elist]\n",
        "        imports_hashed = FeatureHasher(1024, input_type=\"string\").transform([imports]).toarray()[0]\n",
        "\n",
        "        # Two separate elements: libraries (alone) and fully-qualified names of imported functions\n",
        "        return np.hstack([libraries_hashed, imports_hashed]).astype(np.float32)\n",
        "\n",
        "\n",
        "class ExportsInfo(FeatureType):\n",
        "    ''' Information about exported functions. Note that the total number of exported\n",
        "    functions is contained in GeneralFileInfo.\n",
        "    '''\n",
        "\n",
        "    name = 'exports'\n",
        "    dim = 128\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        if lief_binary is None:\n",
        "            return []\n",
        "\n",
        "        # Clipping assumes there are diminishing returns on the discriminatory power of exports beyond\n",
        "        #  the first 10000 characters, and this will help limit the dataset size\n",
        "        if LIEF_EXPORT_OBJECT:\n",
        "            # export is an object with .name attribute (0.10.0 and later)\n",
        "            clipped_exports = [export.name[:10000] for export in lief_binary.exported_functions]\n",
        "        else:\n",
        "            # export is a string (LIEF 0.9.0 and earlier)\n",
        "            clipped_exports = [export[:10000] for export in lief_binary.exported_functions]\n",
        "        \n",
        "\n",
        "        return clipped_exports\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        exports_hashed = FeatureHasher(128, input_type=\"string\").transform([raw_obj]).toarray()[0]\n",
        "        return exports_hashed.astype(np.float32)\n",
        "\n",
        "\n",
        "class GeneralFileInfo(FeatureType):\n",
        "    ''' General information about the file '''\n",
        "\n",
        "    name = 'general'\n",
        "    dim = 10\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        if lief_binary is None:\n",
        "            return {\n",
        "                'size': len(bytez),\n",
        "                'vsize': 0,\n",
        "                'has_debug': 0,\n",
        "                'exports': 0,\n",
        "                'imports': 0,\n",
        "                'has_relocations': 0,\n",
        "                'has_resources': 0,\n",
        "                'has_signature': 0,\n",
        "                'has_tls': 0,\n",
        "                'symbols': 0\n",
        "            }\n",
        "\n",
        "        return {\n",
        "            'size': len(bytez),\n",
        "            'vsize': lief_binary.virtual_size,\n",
        "            'has_debug': int(lief_binary.has_debug),\n",
        "            'exports': len(lief_binary.exported_functions),\n",
        "            'imports': len(lief_binary.imported_functions),\n",
        "            'has_relocations': int(lief_binary.has_relocations),\n",
        "            'has_resources': int(lief_binary.has_resources),\n",
        "            'has_signature': int(lief_binary.has_signatures) if LIEF_HAS_SIGNATURE else int(lief_binary.has_signature),\n",
        "            'has_tls': int(lief_binary.has_tls),\n",
        "            'symbols': len(lief_binary.symbols),\n",
        "        }\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        return np.asarray([\n",
        "            raw_obj['size'], raw_obj['vsize'], raw_obj['has_debug'], raw_obj['exports'], raw_obj['imports'],\n",
        "            raw_obj['has_relocations'], raw_obj['has_resources'], raw_obj['has_signature'], raw_obj['has_tls'],\n",
        "            raw_obj['symbols']\n",
        "        ],\n",
        "                          dtype=np.float32)\n",
        "\n",
        "\n",
        "class HeaderFileInfo(FeatureType):\n",
        "    ''' Machine, architecure, OS, linker and other information extracted from header '''\n",
        "\n",
        "    name = 'header'\n",
        "    dim = 62\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        raw_obj = {}\n",
        "        raw_obj['coff'] = {'timestamp': 0, 'machine': \"\", 'characteristics': []}\n",
        "        raw_obj['optional'] = {\n",
        "            'subsystem': \"\",\n",
        "            'dll_characteristics': [],\n",
        "            'magic': \"\",\n",
        "            'major_image_version': 0,\n",
        "            'minor_image_version': 0,\n",
        "            'major_linker_version': 0,\n",
        "            'minor_linker_version': 0,\n",
        "            'major_operating_system_version': 0,\n",
        "            'minor_operating_system_version': 0,\n",
        "            'major_subsystem_version': 0,\n",
        "            'minor_subsystem_version': 0,\n",
        "            'sizeof_code': 0,\n",
        "            'sizeof_headers': 0,\n",
        "            'sizeof_heap_commit': 0\n",
        "        }\n",
        "        if lief_binary is None:\n",
        "            return raw_obj\n",
        "\n",
        "        raw_obj['coff']['timestamp'] = lief_binary.header.time_date_stamps\n",
        "        raw_obj['coff']['machine'] = str(lief_binary.header.machine).split('.')[-1]\n",
        "        raw_obj['coff']['characteristics'] = [str(c).split('.')[-1] for c in lief_binary.header.characteristics_list]\n",
        "        raw_obj['optional']['subsystem'] = str(lief_binary.optional_header.subsystem).split('.')[-1]\n",
        "        raw_obj['optional']['dll_characteristics'] = [\n",
        "            str(c).split('.')[-1] for c in lief_binary.optional_header.dll_characteristics_lists\n",
        "        ]\n",
        "        raw_obj['optional']['magic'] = str(lief_binary.optional_header.magic).split('.')[-1]\n",
        "        raw_obj['optional']['major_image_version'] = lief_binary.optional_header.major_image_version\n",
        "        raw_obj['optional']['minor_image_version'] = lief_binary.optional_header.minor_image_version\n",
        "        raw_obj['optional']['major_linker_version'] = lief_binary.optional_header.major_linker_version\n",
        "        raw_obj['optional']['minor_linker_version'] = lief_binary.optional_header.minor_linker_version\n",
        "        raw_obj['optional'][\n",
        "            'major_operating_system_version'] = lief_binary.optional_header.major_operating_system_version\n",
        "        raw_obj['optional'][\n",
        "            'minor_operating_system_version'] = lief_binary.optional_header.minor_operating_system_version\n",
        "        raw_obj['optional']['major_subsystem_version'] = lief_binary.optional_header.major_subsystem_version\n",
        "        raw_obj['optional']['minor_subsystem_version'] = lief_binary.optional_header.minor_subsystem_version\n",
        "        raw_obj['optional']['sizeof_code'] = lief_binary.optional_header.sizeof_code\n",
        "        raw_obj['optional']['sizeof_headers'] = lief_binary.optional_header.sizeof_headers\n",
        "        raw_obj['optional']['sizeof_heap_commit'] = lief_binary.optional_header.sizeof_heap_commit\n",
        "        return raw_obj\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        return np.hstack([\n",
        "            raw_obj['coff']['timestamp'],\n",
        "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['coff']['machine']]]).toarray()[0],\n",
        "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['coff']['characteristics']]).toarray()[0],\n",
        "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['subsystem']]]).toarray()[0],\n",
        "            FeatureHasher(10, input_type=\"string\").transform([raw_obj['optional']['dll_characteristics']]).toarray()[0],\n",
        "            FeatureHasher(10, input_type=\"string\").transform([[raw_obj['optional']['magic']]]).toarray()[0],\n",
        "            raw_obj['optional']['major_image_version'],\n",
        "            raw_obj['optional']['minor_image_version'],\n",
        "            raw_obj['optional']['major_linker_version'],\n",
        "            raw_obj['optional']['minor_linker_version'],\n",
        "            raw_obj['optional']['major_operating_system_version'],\n",
        "            raw_obj['optional']['minor_operating_system_version'],\n",
        "            raw_obj['optional']['major_subsystem_version'],\n",
        "            raw_obj['optional']['minor_subsystem_version'],\n",
        "            raw_obj['optional']['sizeof_code'],\n",
        "            raw_obj['optional']['sizeof_headers'],\n",
        "            raw_obj['optional']['sizeof_heap_commit'],\n",
        "        ]).astype(np.float32)\n",
        "\n",
        "\n",
        "class StringExtractor(FeatureType):\n",
        "    ''' Extracts strings from raw byte stream '''\n",
        "\n",
        "    name = 'strings'\n",
        "    dim = 1 + 1 + 1 + 96 + 1 + 1 + 1 + 1 + 1\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "        # all consecutive runs of 0x20 - 0x7f that are 5+ characters\n",
        "        self._allstrings = re.compile(b'[\\x20-\\x7f]{5,}')\n",
        "        # occurances of the string 'C:\\'.  Not actually extracting the path\n",
        "        self._paths = re.compile(b'c:\\\\\\\\', re.IGNORECASE)\n",
        "        # occurances of http:// or https://.  Not actually extracting the URLs\n",
        "        self._urls = re.compile(b'https?://', re.IGNORECASE)\n",
        "        # occurances of the string prefix HKEY_.  No actually extracting registry names\n",
        "        self._registry = re.compile(b'HKEY_')\n",
        "        # crude evidence of an MZ header (dropper?) somewhere in the byte stream\n",
        "        self._mz = re.compile(b'MZ')\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        allstrings = self._allstrings.findall(bytez)\n",
        "        if allstrings:\n",
        "            # statistics about strings:\n",
        "            string_lengths = [len(s) for s in allstrings]\n",
        "            avlength = sum(string_lengths) / len(string_lengths)\n",
        "            # map printable characters 0x20 - 0x7f to an int array consisting of 0-95, inclusive\n",
        "            as_shifted_string = [b - ord(b'\\x20') for b in b''.join(allstrings)]\n",
        "            c = np.bincount(as_shifted_string, minlength=96)  # histogram count\n",
        "            # distribution of characters in printable strings\n",
        "            csum = c.sum()\n",
        "            p = c.astype(np.float32) / csum\n",
        "            wh = np.where(c)[0]\n",
        "            H = np.sum(-p[wh] * np.log2(p[wh]))  # entropy\n",
        "        else:\n",
        "            avlength = 0\n",
        "            c = np.zeros((96,), dtype=np.float32)\n",
        "            H = 0\n",
        "            csum = 0\n",
        "\n",
        "        return {\n",
        "            'numstrings': len(allstrings),\n",
        "            'avlength': avlength,\n",
        "            'printabledist': c.tolist(),  # store non-normalized histogram\n",
        "            'printables': int(csum),\n",
        "            'entropy': float(H),\n",
        "            'paths': len(self._paths.findall(bytez)),\n",
        "            'urls': len(self._urls.findall(bytez)),\n",
        "            'registry': len(self._registry.findall(bytez)),\n",
        "            'MZ': len(self._mz.findall(bytez))\n",
        "        }\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        hist_divisor = float(raw_obj['printables']) if raw_obj['printables'] > 0 else 1.0\n",
        "        return np.hstack([\n",
        "            raw_obj['numstrings'], raw_obj['avlength'], raw_obj['printables'],\n",
        "            np.asarray(raw_obj['printabledist']) / hist_divisor, raw_obj['entropy'], raw_obj['paths'], raw_obj['urls'],\n",
        "            raw_obj['registry'], raw_obj['MZ']\n",
        "        ]).astype(np.float32)\n",
        "\n",
        "\n",
        "class DataDirectories(FeatureType):\n",
        "    ''' Extracts size and virtual address of the first 15 data directories '''\n",
        "\n",
        "    name = 'datadirectories'\n",
        "    dim = 15 * 2\n",
        "\n",
        "    def __init__(self):\n",
        "        super(FeatureType, self).__init__()\n",
        "        self._name_order = [\n",
        "            \"EXPORT_TABLE\", \"IMPORT_TABLE\", \"RESOURCE_TABLE\", \"EXCEPTION_TABLE\", \"CERTIFICATE_TABLE\",\n",
        "            \"BASE_RELOCATION_TABLE\", \"DEBUG\", \"ARCHITECTURE\", \"GLOBAL_PTR\", \"TLS_TABLE\", \"LOAD_CONFIG_TABLE\",\n",
        "            \"BOUND_IMPORT\", \"IAT\", \"DELAY_IMPORT_DESCRIPTOR\", \"CLR_RUNTIME_HEADER\"\n",
        "        ]\n",
        "\n",
        "    def raw_features(self, bytez, lief_binary):\n",
        "        output = []\n",
        "        if lief_binary is None:\n",
        "            return output\n",
        "\n",
        "        for data_directory in lief_binary.data_directories:\n",
        "            output.append({\n",
        "                \"name\": str(data_directory.type).replace(\"DATA_DIRECTORY.\", \"\"),\n",
        "                \"size\": data_directory.size,\n",
        "                \"virtual_address\": data_directory.rva\n",
        "            })\n",
        "        return output\n",
        "\n",
        "    def process_raw_features(self, raw_obj):\n",
        "        features = np.zeros(2 * len(self._name_order), dtype=np.float32)\n",
        "        for i in range(len(self._name_order)):\n",
        "            if i < len(raw_obj):\n",
        "                features[2 * i] = raw_obj[i][\"size\"]\n",
        "                features[2 * i + 1] = raw_obj[i][\"virtual_address\"]\n",
        "        return features\n",
        "\n"
      ],
      "metadata": {
        "id": "s7EYf76W8pkb"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip \"854137.exe.zip\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4eGJirU9JtM",
        "outputId": "57837211-55ed-4e4c-f0e0-f19d8bc929b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  854137.exe.zip\n",
            "   skipping: 854137.exe              need PK compat. v5.1 (can do v4.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "fqGHAZm6N3ZA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}